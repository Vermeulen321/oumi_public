{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vermeulen321/oumi_public/blob/main/notebooks/Oumi%20-%20A%20Tour.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJLNYxbBfS2_"
      },
      "source": [
        "<div class=\"align-center\">\n",
        "<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n",
        "\n",
        "[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n",
        "[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n",
        "[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n",
        "</div>\n",
        "\n",
        "üëã Welcome to Open Universal Machine Intelligence (Oumi)!\n",
        "\n",
        "üöÄ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](hhttps://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n",
        "\n",
        "ü§ù Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n",
        "\n",
        "‚≠ê If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7bYaH10SgtN"
      },
      "source": [
        "# A Tour of Oumi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkhNGqE1SgtP"
      },
      "source": [
        "This tutorial will give you a brief overview of Oumi's core functionality. We'll cover:\n",
        "\n",
        "1. Training a model\n",
        "1. Performing model inference\n",
        "1. Evaluating a model against common benchmarks\n",
        "1. Launching jobs\n",
        "1. Customizing datasets and clouds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHDr11SqSgtP"
      },
      "source": [
        "# üìã Prerequisites\n",
        "## Oumi Installation\n",
        "\n",
        "First, let's install Oumi. You can find more detailed instructions [here](https://oumi.ai/docs/en/latest/get_started/installation.html).\n",
        "\n",
        "If you have a GPU, you can run the following commands to install Oumi:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUvmB3aEfS3B",
        "outputId": "6aa5fc42-ea00-448d-fbe4-3c3e66b96a9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[2mUsing Python 3.11.8 environment at: /Users/oussamaelachqar/miniconda3/envs/oumi\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 15ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%pip install uv -q\n",
        "!uv pip install oumi --no-progress --system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOXCK588fS3C"
      },
      "source": [
        "‚ùó**WARNING:** After the first `pip install`, you may have to restart the notebook for the package updates to take effect (Colab Menu: `Runtime` -> `Restart Session`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JPmWKRVCSgtP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "tutorial_dir = \"tour_tutorial\"\n",
        "\n",
        "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable warnings from HF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8PTJuc4SgtQ"
      },
      "source": [
        "# ‚öíÔ∏è Training a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2_HamuySgtQ"
      },
      "source": [
        "Oumi supports training both custom and out-of-the-box models. Want to try out a model on HuggingFace? You can do that. Want to train your own custom Pytorch model? No problem.\n",
        "\n",
        "## A Quick Demo\n",
        "\n",
        "Let's try training a pre-existing model on HuggingFace. We'll use SmolLM2 135M as it's small and trains quickly.\n",
        "\n",
        "Oumi uses [training configuration files](https://oumi.ai/docs/en/latest/api/oumi.core.configs.html#oumi.core.configs.TrainingConfig) to specify training parameters. We've already created a training config for SmolLM2 ‚Äî let's give it a try!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l2SQ9fZiSgtQ"
      },
      "outputs": [],
      "source": [
        "yaml_content = f\"\"\"\n",
        "model:\n",
        "  model_name: \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  trust_remote_code: True\n",
        "\n",
        "data:\n",
        "  train:\n",
        "    datasets:\n",
        "      - dataset_name: \"yahma/alpaca-cleaned\"\n",
        "    target_col: \"prompt\"\n",
        "\n",
        "training:\n",
        "  trainer_type: \"TRL_SFT\"\n",
        "  per_device_train_batch_size: 2\n",
        "  max_steps: 10 # Quick \"mini\" training, for demo purposes only.\n",
        "  run_name: \"smollm2_135m_sft\"\n",
        "  output_dir: \"{tutorial_dir}/output\"\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{tutorial_dir}/train.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2GpQDGG5SgtQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eff1ab7b-971e-4873-d482-da5a2352f954"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-02-04 05:45:07,571][oumi][rank0][pid:5475][MainThread][INFO]][torch_utils.py:66] Torch version: 2.4.1+cu121. NumPy version: 1.26.4\n",
            "[2025-02-04 05:45:07,574][oumi][rank0][pid:5475][MainThread][INFO]][torch_utils.py:68] CUDA is not available!\n",
            "[2025-02-04 05:45:07,583][oumi][rank0][pid:5475][MainThread][INFO]][train.py:133] Oumi version: 0.1.3\n",
            "[2025-02-04 05:45:07,591][oumi][rank0][pid:5475][MainThread][INFO]][train.py:174] TrainingConfig:\n",
            "TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='yahma/alpaca-cleaned',\n",
            "                                                                                dataset_path=None,\n",
            "                                                                                subset=None,\n",
            "                                                                                split='train',\n",
            "                                                                                dataset_kwargs={},\n",
            "                                                                                sample_count=None,\n",
            "                                                                                mixture_proportion=None,\n",
            "                                                                                shuffle=False,\n",
            "                                                                                seed=None,\n",
            "                                                                                shuffle_buffer_size=1000,\n",
            "                                                                                trust_remote_code=False,\n",
            "                                                                                transform_num_workers=None)],\n",
            "                                                        collator_name=None,\n",
            "                                                        pack=False,\n",
            "                                                        stream=False,\n",
            "                                                        target_col='prompt',\n",
            "                                                        mixture_strategy='first_exhausted',\n",
            "                                                        seed=None,\n",
            "                                                        use_async_dataset=False,\n",
            "                                                        use_torchdata=None),\n",
            "                               test=DatasetSplitParams(datasets=[],\n",
            "                                                       collator_name=None,\n",
            "                                                       pack=False,\n",
            "                                                       stream=False,\n",
            "                                                       target_col=None,\n",
            "                                                       mixture_strategy='first_exhausted',\n",
            "                                                       seed=None,\n",
            "                                                       use_async_dataset=False,\n",
            "                                                       use_torchdata=None),\n",
            "                               validation=DatasetSplitParams(datasets=[],\n",
            "                                                             collator_name=None,\n",
            "                                                             pack=False,\n",
            "                                                             stream=False,\n",
            "                                                             target_col=None,\n",
            "                                                             mixture_strategy='first_exhausted',\n",
            "                                                             seed=None,\n",
            "                                                             use_async_dataset=False,\n",
            "                                                             use_torchdata=None)),\n",
            "               model=ModelParams(model_name='HuggingFaceTB/SmolLM2-135M-Instruct',\n",
            "                                 adapter_model=None,\n",
            "                                 tokenizer_name=None,\n",
            "                                 tokenizer_pad_token=None,\n",
            "                                 tokenizer_kwargs={},\n",
            "                                 model_max_length=None,\n",
            "                                 load_pretrained_weights=True,\n",
            "                                 trust_remote_code=True,\n",
            "                                 torch_dtype_str='bfloat16',\n",
            "                                 compile=False,\n",
            "                                 chat_template=None,\n",
            "                                 attn_implementation=None,\n",
            "                                 device_map='auto',\n",
            "                                 model_kwargs={},\n",
            "                                 enable_liger_kernel=False,\n",
            "                                 shard_for_eval=False,\n",
            "                                 freeze_layers=[]),\n",
            "               training=TrainingParams(use_peft=False,\n",
            "                                       trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>,\n",
            "                                       enable_gradient_checkpointing=False,\n",
            "                                       gradient_checkpointing_kwargs={},\n",
            "                                       output_dir='tour_tutorial/output',\n",
            "                                       per_device_train_batch_size=2,\n",
            "                                       per_device_eval_batch_size=8,\n",
            "                                       gradient_accumulation_steps=1,\n",
            "                                       max_steps=10,\n",
            "                                       num_train_epochs=3,\n",
            "                                       save_epoch=False,\n",
            "                                       save_steps=500,\n",
            "                                       save_final_model=True,\n",
            "                                       seed=42,\n",
            "                                       run_name='smollm2_135m_sft',\n",
            "                                       metrics_function=None,\n",
            "                                       log_level='info',\n",
            "                                       dep_log_level='warning',\n",
            "                                       enable_wandb=False,\n",
            "                                       enable_tensorboard=True,\n",
            "                                       logging_strategy='steps',\n",
            "                                       logging_dir=None,\n",
            "                                       logging_steps=50,\n",
            "                                       logging_first_step=False,\n",
            "                                       eval_strategy='no',\n",
            "                                       eval_steps=500,\n",
            "                                       learning_rate=5e-05,\n",
            "                                       lr_scheduler_type='linear',\n",
            "                                       lr_scheduler_kwargs={},\n",
            "                                       warmup_ratio=None,\n",
            "                                       warmup_steps=None,\n",
            "                                       optimizer='adamw_torch',\n",
            "                                       weight_decay=0.0,\n",
            "                                       adam_beta1=0.9,\n",
            "                                       adam_beta2=0.999,\n",
            "                                       adam_epsilon=1e-08,\n",
            "                                       sgd_momentum=0.0,\n",
            "                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,\n",
            "                                       compile=False,\n",
            "                                       include_performance_metrics=False,\n",
            "                                       include_alternative_mfu_metrics=False,\n",
            "                                       log_model_summary=False,\n",
            "                                       resume_from_checkpoint=None,\n",
            "                                       try_resume_from_last_checkpoint=False,\n",
            "                                       dataloader_num_workers=0,\n",
            "                                       dataloader_prefetch_factor=None,\n",
            "                                       dataloader_main_process_only=None,\n",
            "                                       ddp_find_unused_parameters=None,\n",
            "                                       max_grad_norm=1.0,\n",
            "                                       trainer_kwargs={},\n",
            "                                       profiler=ProfilerParams(save_dir=None,\n",
            "                                                               enable_cpu_profiling=False,\n",
            "                                                               enable_cuda_profiling=False,\n",
            "                                                               record_shapes=False,\n",
            "                                                               profile_memory=False,\n",
            "                                                               with_stack=False,\n",
            "                                                               with_flops=False,\n",
            "                                                               with_modules=False,\n",
            "                                                               row_limit=50,\n",
            "                                                               schedule=ProfilerScheduleParams(enable_schedule=False,\n",
            "                                                                                               wait=0,\n",
            "                                                                                               warmup=1,\n",
            "                                                                                               active=3,\n",
            "                                                                                               repeat=1,\n",
            "                                                                                               skip_first=1)),\n",
            "                                       telemetry=TelemetryParams(telemetry_dir='telemetry',\n",
            "                                                                 collect_telemetry_for_all_ranks=False,\n",
            "                                                                 track_gpu_temperature=False),\n",
            "                                       empty_device_cache_steps=None,\n",
            "                                       nccl_default_timeout_minutes=None),\n",
            "               peft=PeftParams(lora_r=8,\n",
            "                               lora_alpha=8,\n",
            "                               lora_dropout=0.0,\n",
            "                               lora_target_modules=None,\n",
            "                               lora_modules_to_save=None,\n",
            "                               lora_bias='none',\n",
            "                               lora_init_weights=<LoraWeightInitialization.DEFAULT: 'default'>,\n",
            "                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
            "                               q_lora=False,\n",
            "                               q_lora_bits=4,\n",
            "                               bnb_4bit_quant_type='fp4',\n",
            "                               use_bnb_nested_quant=False,\n",
            "                               bnb_4bit_quant_storage='uint8',\n",
            "                               bnb_4bit_compute_dtype='float32',\n",
            "                               peft_save_mode=<PeftSaveMode.ADAPTER_ONLY: 'adapter_only'>),\n",
            "               fsdp=FSDPParams(enable_fsdp=False,\n",
            "                               sharding_strategy=<ShardingStrategy.FULL_SHARD: 'FULL_SHARD'>,\n",
            "                               cpu_offload=False,\n",
            "                               mixed_precision=None,\n",
            "                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,\n",
            "                               forward_prefetch=False,\n",
            "                               use_orig_params=None,\n",
            "                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,\n",
            "                               auto_wrap_policy=<AutoWrapPolicy.NO_WRAP: 'NO_WRAP'>,\n",
            "                               min_num_params=100000,\n",
            "                               transformer_layer_cls=None,\n",
            "                               sync_module_states=True))\n",
            "[2025-02-04 05:45:09,618][oumi][rank0][pid:5475][MainThread][INFO]][models.py:185] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
            "[2025-02-04 05:45:09,620][oumi][rank0][pid:5475][MainThread][INFO]][models.py:255] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
            "[2025-02-04 05:45:10,273][oumi][rank0][pid:5475][MainThread][INFO]][base_map_dataset.py:68] Creating map dataset (type: AlpacaDataset) dataset_name: 'yahma/alpaca-cleaned', dataset_path: 'None'...\n",
            "[2025-02-04 05:45:11,987][oumi][rank0][pid:5475][MainThread][INFO]][base_map_dataset.py:472] Dataset Info:\n",
            "\tSplit: train\n",
            "\tVersion: 0.0.0\n",
            "\tDataset size: 40283906\n",
            "\tDownload size: 44307561\n",
            "\tSize: 84591467 bytes\n",
            "\tRows: 51760\n",
            "\tColumns: ['output', 'input', 'instruction']\n",
            "[2025-02-04 05:45:12,489][oumi][rank0][pid:5475][MainThread][INFO]][base_map_dataset.py:411] Loaded DataFrame with shape: (51760, 3). Columns:\n",
            "output         object\n",
            "input          object\n",
            "instruction    object\n",
            "dtype: object\n",
            "[2025-02-04 05:45:12,562][oumi][rank0][pid:5475][MainThread][INFO]][base_map_dataset.py:297] AlpacaDataset: features=dict_keys(['input_ids', 'attention_mask'])\n",
            "[2025-02-04 05:45:13,350][oumi][rank0][pid:5475][MainThread][INFO]][base_map_dataset.py:361] Finished transforming dataset (AlpacaDataset)! Speed: 65946.60 examples/sec. Examples: 51760. Duration: 0.8 sec. Transform workers: 1.\n",
            "[2025-02-04 05:45:13,815][oumi][rank0][pid:5475][MainThread][INFO]][torch_profiler_utils.py:150] PROF: Torch Profiler disabled!\n",
            "[2025-02-04 05:45:13,823][oumi][rank0][pid:5475][MainThread][INFO]][training.py:49] SFTConfig(output_dir='tour_tutorial/output',\n",
            "          overwrite_output_dir=False,\n",
            "          do_train=False,\n",
            "          do_eval=False,\n",
            "          do_predict=False,\n",
            "          eval_strategy=<IntervalStrategy.NO: 'no'>,\n",
            "          prediction_loss_only=False,\n",
            "          per_device_train_batch_size=2,\n",
            "          per_device_eval_batch_size=8,\n",
            "          per_gpu_train_batch_size=None,\n",
            "          per_gpu_eval_batch_size=None,\n",
            "          gradient_accumulation_steps=1,\n",
            "          eval_accumulation_steps=None,\n",
            "          eval_delay=0,\n",
            "          torch_empty_cache_steps=None,\n",
            "          learning_rate=5e-05,\n",
            "          weight_decay=0.0,\n",
            "          adam_beta1=0.9,\n",
            "          adam_beta2=0.999,\n",
            "          adam_epsilon=1e-08,\n",
            "          max_grad_norm=1.0,\n",
            "          num_train_epochs=3,\n",
            "          max_steps=10,\n",
            "          lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>,\n",
            "          lr_scheduler_kwargs={},\n",
            "          warmup_ratio=0.0,\n",
            "          warmup_steps=0,\n",
            "          log_level='warning',\n",
            "          log_level_replica='warning',\n",
            "          log_on_each_node=True,\n",
            "          logging_dir='tour_tutorial/output/runs/Feb04_05-45-13_3230a9be9608',\n",
            "          logging_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
            "          logging_first_step=False,\n",
            "          logging_steps=50,\n",
            "          logging_nan_inf_filter=True,\n",
            "          save_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
            "          save_steps=500,\n",
            "          save_total_limit=None,\n",
            "          save_safetensors=True,\n",
            "          save_on_each_node=False,\n",
            "          save_only_model=False,\n",
            "          restore_callback_states_from_checkpoint=False,\n",
            "          no_cuda=False,\n",
            "          use_cpu=False,\n",
            "          use_mps_device=False,\n",
            "          seed=42,\n",
            "          data_seed=None,\n",
            "          jit_mode_eval=False,\n",
            "          use_ipex=False,\n",
            "          bf16=False,\n",
            "          fp16=False,\n",
            "          fp16_opt_level='O1',\n",
            "          half_precision_backend='auto',\n",
            "          bf16_full_eval=False,\n",
            "          fp16_full_eval=False,\n",
            "          tf32=None,\n",
            "          local_rank=0,\n",
            "          ddp_backend=None,\n",
            "          tpu_num_cores=None,\n",
            "          tpu_metrics_debug=False,\n",
            "          debug=[],\n",
            "          dataloader_drop_last=False,\n",
            "          eval_steps=500,\n",
            "          dataloader_num_workers=0,\n",
            "          dataloader_prefetch_factor=None,\n",
            "          past_index=-1,\n",
            "          run_name='smollm2_135m_sft',\n",
            "          disable_tqdm=False,\n",
            "          remove_unused_columns=True,\n",
            "          label_names=None,\n",
            "          load_best_model_at_end=False,\n",
            "          metric_for_best_model=None,\n",
            "          greater_is_better=None,\n",
            "          ignore_data_skip=False,\n",
            "          fsdp=[],\n",
            "          fsdp_min_num_params=0,\n",
            "          fsdp_config={'min_num_params': 0,\n",
            "                       'xla': False,\n",
            "                       'xla_fsdp_grad_ckpt': False,\n",
            "                       'xla_fsdp_v2': False},\n",
            "          fsdp_transformer_layer_cls_to_wrap=None,\n",
            "          accelerator_config=AcceleratorConfig(split_batches=False,\n",
            "                                               dispatch_batches=None,\n",
            "                                               even_batches=True,\n",
            "                                               use_seedable_sampler=True,\n",
            "                                               non_blocking=False,\n",
            "                                               gradient_accumulation_kwargs=None,\n",
            "                                               use_configured_state=False),\n",
            "          deepspeed=None,\n",
            "          label_smoothing_factor=0.0,\n",
            "          optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>,\n",
            "          optim_args=None,\n",
            "          adafactor=False,\n",
            "          group_by_length=False,\n",
            "          length_column_name='length',\n",
            "          report_to=['tensorboard'],\n",
            "          ddp_find_unused_parameters=None,\n",
            "          ddp_bucket_cap_mb=None,\n",
            "          ddp_broadcast_buffers=None,\n",
            "          dataloader_pin_memory=True,\n",
            "          dataloader_persistent_workers=False,\n",
            "          skip_memory_metrics=True,\n",
            "          use_legacy_prediction_loop=False,\n",
            "          push_to_hub=False,\n",
            "          resume_from_checkpoint=None,\n",
            "          hub_model_id=None,\n",
            "          hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>,\n",
            "          hub_token=None,\n",
            "          hub_private_repo=False,\n",
            "          hub_always_push=False,\n",
            "          gradient_checkpointing=False,\n",
            "          gradient_checkpointing_kwargs={},\n",
            "          include_inputs_for_metrics=False,\n",
            "          eval_do_concat_batches=True,\n",
            "          fp16_backend='auto',\n",
            "          evaluation_strategy=None,\n",
            "          push_to_hub_model_id=None,\n",
            "          push_to_hub_organization=None,\n",
            "          push_to_hub_token=None,\n",
            "          mp_parameters='',\n",
            "          auto_find_batch_size=False,\n",
            "          full_determinism=False,\n",
            "          torchdynamo=None,\n",
            "          ray_scope='last',\n",
            "          ddp_timeout=1800,\n",
            "          torch_compile=False,\n",
            "          torch_compile_backend=None,\n",
            "          torch_compile_mode=None,\n",
            "          dispatch_batches=None,\n",
            "          split_batches=None,\n",
            "          include_tokens_per_second=False,\n",
            "          include_num_input_tokens_seen=False,\n",
            "          neftune_noise_alpha=None,\n",
            "          optim_target_modules=None,\n",
            "          batch_eval_metrics=False,\n",
            "          eval_on_start=False,\n",
            "          use_liger_kernel=False,\n",
            "          eval_use_gather_object=False,\n",
            "          dataset_text_field=None,\n",
            "          packing=False,\n",
            "          max_seq_length=None,\n",
            "          dataset_num_proc=None,\n",
            "          dataset_batch_size=1000,\n",
            "          model_init_kwargs=None,\n",
            "          dataset_kwargs=None,\n",
            "          eval_packing=None,\n",
            "          num_of_sequences=1024,\n",
            "          chars_per_token=3.6,\n",
            "          use_liger=False)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-02-04 05:45:13,852][oumi][rank0][pid:5475][MainThread][INFO]][device_utils.py:283] GPU Metrics Before Training: GPU runtime info: None.\n",
            "[2025-02-04 05:45:13,855][oumi][rank0][pid:5475][MainThread][INFO]][train.py:312] Training init time: 6.284s\n",
            "[2025-02-04 05:45:13,860][oumi][rank0][pid:5475][MainThread][INFO]][train.py:313] Starting training... (TrainerType.TRL_SFT, transformers: 4.45.2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 6/10 25:38 < 25:38, 0.00 it/s, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 1:05:01, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-02-04 06:56:38,965][oumi][rank0][pid:5475][MainThread][INFO]][train.py:320] Training is Complete.\n",
            "[2025-02-04 06:56:38,972][oumi][rank0][pid:5475][MainThread][INFO]][device_utils.py:283] GPU Metrics After Training: GPU runtime info: None.\n",
            "[2025-02-04 06:56:38,976][oumi][rank0][pid:5475][MainThread][INFO]][train.py:327] Saving final state...\n",
            "[2025-02-04 06:56:38,981][oumi][rank0][pid:5475][MainThread][INFO]][train.py:332] Saving final model...\n",
            "[2025-02-04 06:56:40,816][oumi][rank0][pid:5475][MainThread][INFO]][hf_trainer.py:102] Model has been saved at tour_tutorial/output\n",
            "[2025-02-04 06:56:40,822][oumi][rank0][pid:5475][MainThread][INFO]][train.py:339] \n",
            "\n",
            "¬ª We're always looking for feedback. What's one thing we can improve? https://oumi.ai/feedback\n"
          ]
        }
      ],
      "source": [
        "from oumi.core.configs import TrainingConfig\n",
        "from oumi.train import train\n",
        "\n",
        "config = TrainingConfig.from_yaml(str(Path(tutorial_dir) / \"train.yaml\"))\n",
        "\n",
        "train(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XfsWKEFSgtR"
      },
      "source": [
        "Congratulations, you've trained your first model using Oumi!\n",
        "\n",
        "You can also train your own custom Pytorch model. We cover that in depth in our [Finetuning Tutorial](https://github.com/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Finetuning%20Tutorial.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdMUxYZcSgtR"
      },
      "source": [
        "# üß† Model Inference\n",
        "\n",
        "Now that you've trained a model, let's run inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z4bx6ibXSgtR"
      },
      "outputs": [],
      "source": [
        "yaml_content = f\"\"\"\n",
        "model:\n",
        "  model_name: \"{tutorial_dir}/output\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "\n",
        "generation:\n",
        "  max_new_tokens: 128\n",
        "  batch_size: 1\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{tutorial_dir}/infer.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XaxxRD1sSgtR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d71678ef-ce4c-4b7d-c9cf-8e1e4f93c086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-02-04 07:02:21,709][oumi][rank0][pid:5475][MainThread][WARNING]][infer.py:19] No inference engine specified. Using the default 'native' engine.\n",
            "[2025-02-04 07:02:21,714][oumi][rank0][pid:5475][MainThread][INFO]][models.py:185] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
            "[2025-02-04 07:02:21,719][oumi][rank0][pid:5475][MainThread][INFO]][models.py:255] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
            "[2025-02-04 07:02:22,745][oumi][rank0][pid:5475][MainThread][INFO]][native_text_inference_engine.py:111] Setting EOS token id to `2`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conversation_id=None messages=[USER: Remember that we didn't train for long, so the results might not be great., ASSISTANT: I'm sorry for the inconvenience, but as a chatbot, I don't have the ability to access or process data from external sources. I'm designed to provide information and guidance based on the information I receive from users. I'm designed to be helpful and informative, but I don't have the capability to access or process data from external sources. If you have any specific questions or need help with a particular topic, feel free to ask.] metadata={}\n"
          ]
        }
      ],
      "source": [
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.infer import infer\n",
        "\n",
        "config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"infer.yaml\"))\n",
        "\n",
        "input_text = (\n",
        "    \"Remember that we didn't train for long, so the results might not be great.\"\n",
        ")\n",
        "\n",
        "results = infer(config=config, inputs=[input_text])\n",
        "\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmlA3ynCSgtR"
      },
      "source": [
        "We can also run inference using the pretrained model by slightly tweaking our config:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Wbo2QAxoSgtR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d0d3382-224a-4424-8778-5729bd69c63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-02-04 07:21:13,114][oumi][rank0][pid:5475][MainThread][WARNING]][infer.py:19] No inference engine specified. Using the default 'native' engine.\n",
            "[2025-02-04 07:21:13,116][oumi][rank0][pid:5475][MainThread][INFO]][models.py:185] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
            "[2025-02-04 07:21:13,245][oumi][rank0][pid:5475][MainThread][INFO]][models.py:255] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
            "[2025-02-04 07:21:14,272][oumi][rank0][pid:5475][MainThread][INFO]][native_text_inference_engine.py:111] Setting EOS token id to `2`\n",
            "conversation_id=None messages=[USER: Input for the pretrained model: What is your name? , ASSISTANT: My name is Alex Chen. I'm a data scientist and AI assistant, trained on a vast dataset of text data, which I use to train my models for various tasks.] metadata={}\n"
          ]
        }
      ],
      "source": [
        "base_model_config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"infer.yaml\"))\n",
        "base_model_config.model.model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "\n",
        "input_text = \"Input for the pretrained model: What is your name? \"\n",
        "\n",
        "results = infer(config=base_model_config, inputs=[input_text])\n",
        "\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnxsJBCTSgtR"
      },
      "source": [
        "# üìä Evaluating a Model against Common Benchmarks\n",
        "\n",
        "You can use Oumi to evaluate pretrained and tuned models against standard benchmarks. For example, let's evaluate our tuned model against `Hellaswag`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tArOVDzVSgtR"
      },
      "outputs": [],
      "source": [
        "yaml_content = f\"\"\"\n",
        "model:\n",
        "  model_name: \"{tutorial_dir}/output\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "\n",
        "tasks:\n",
        "  - evaluation_platform: lm_harness\n",
        "    task_name: mmlu_college_computer_science\n",
        "\n",
        "generation:\n",
        "  batch_size: null # This will let LM HARNESS find the maximum possible batch size.\n",
        "output_dir: \"{tutorial_dir}/output/evaluation\"\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{tutorial_dir}/eval.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e-tcwtMhSgtR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "b91a37ed-5cf1-4671-e88f-e9d0b71db3e9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'EvaluationConfig' object has no attribute 'data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5f2b866712cf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Uncomment the following line to run evals against the V1 HuggingFace Leaderboard.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# This may take a while.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0meval_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"huggingface_leaderboard_v1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'EvaluationConfig' object has no attribute 'data'"
          ]
        }
      ],
      "source": [
        "from oumi.core.configs import EvaluationConfig\n",
        "from oumi.evaluate import evaluate\n",
        "\n",
        "eval_config = EvaluationConfig.from_yaml(str(Path(tutorial_dir) / \"eval.yaml\"))\n",
        "\n",
        "# Uncomment the following line to run evals against the V1 HuggingFace Leaderboard.\n",
        "# This may take a while.\n",
        "# eval_config.data.datasets[0].dataset_name = \"huggingface_leaderboard_v1\"\n",
        "\n",
        "evaluate(eval_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUnIyaZfSgtS"
      },
      "source": [
        "# ‚òÅÔ∏è Launching Jobs\n",
        "\n",
        "Oftentimes you'll need to run various tasks (training, evaluation, etc.) on remote hardware that's better suited for the task. Oumi can handle this for you by launching jobs on various compute clusters. For more information about running jobs, see our [Running Jobs Remotely tutorial](https://github.com/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Running%20Jobs%20Remotely.ipynb). For running jobs on custom clusters, see our [Launching Jobs on Custom Clusters tutorial](https://github.com/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Launching%20Jobs%20on%20Custom%20Clusters.ipynb).\n",
        "\n",
        "\n",
        "Today, Oumi supports running jobs on several cloud provider platforms.\n",
        "\n",
        "For the latest list, we can run the `which_clouds` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MJipzQl4SgtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6521d11-5cd8-479c-f0f5-b850d4d049e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supported Clouds in Oumi:\n",
            "local\n",
            "polaris\n",
            "runpod\n",
            "gcp\n",
            "lambda\n",
            "aws\n",
            "azure\n"
          ]
        }
      ],
      "source": [
        "import oumi.launcher as launcher\n",
        "\n",
        "print(\"Supported Clouds in Oumi:\")\n",
        "for cloud in launcher.which_clouds():\n",
        "    print(cloud)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VdZqPtpSgtS"
      },
      "source": [
        "Let's run a simple \"Hello World\" job locally to demonstrate how to use the Oumi job launcher. This job will echo `Hello World`, then run the same training job executed above. Running this job on a cloud provider like GCP simply involves changing the `cloud` field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "15_fKEiNSgtS"
      },
      "outputs": [],
      "source": [
        "yaml_content = f\"\"\"\n",
        "name: hello-world\n",
        "resources:\n",
        "  cloud: local\n",
        "\n",
        "working_dir: .\n",
        "\n",
        "envs:\n",
        "  TEST_ENV_VARIABLE: '\"Hello, World!\"'\n",
        "  OUMI_LOGGING_DIR: \"{tutorial_dir}/logs\"\n",
        "\n",
        "run: |\n",
        "  echo \"$TEST_ENV_VARIABLE\"\n",
        "  oumi train -c {tutorial_dir}/train.yaml\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{tutorial_dir}/job.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VA-n5iUASgtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8a68a62-effc-4984-884f-5efd0f47776b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is done!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "job_config = launcher.JobConfig.from_yaml(str(Path(tutorial_dir) / \"job.yaml\"))\n",
        "cluster, job_status = launcher.up(job_config, cluster_name=None)\n",
        "\n",
        "while job_status and not job_status.done:\n",
        "    print(\"Job is running...\")\n",
        "    time.sleep(15)\n",
        "    job_status = cluster.get_job(job_status.id)\n",
        "print(\"Job is done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuVVlpbOSgtS"
      },
      "source": [
        "The job created logs under our tutorial directory. Let's take a look at the directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-BGuSP6XTVKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc0ad4d-abea-4c2c-b6e3-0c31accf1928"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2025_02_04_11_03_17_652_0.stderr', '2025_02_04_11_03_17_652_0.stdout']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "logs_dir = f\"{tutorial_dir}/logs\"\n",
        "os.listdir(logs_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_iz4i4ATXuB"
      },
      "source": [
        "Now let's parse the logfiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RbhUaUchSgtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd157963-a2f4-4c7c-a257-267d106d0f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log file: tour_tutorial/logs/2025_02_04_11_03_17_652_0.stderr\n",
            "2025-02-04 11:03:37.336208: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738667017.647543   82345 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738667017.733658   82345 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\n",
            " 10%|‚ñà         | 1/10 [06:37<59:36, 397.39s/it]\n",
            " 20%|‚ñà‚ñà        | 2/10 [13:25<53:49, 403.66s/it]\n",
            " 30%|‚ñà‚ñà‚ñà       | 3/10 [19:03<43:36, 373.81s/it]\n",
            " 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [27:56<43:39, 436.54s/it]\n",
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [33:40<33:35, 403.17s/it]\n",
            " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [42:20<29:32, 443.01s/it]\n",
            " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [46:51<19:19, 386.63s/it]\n",
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [58:50<16:25, 492.61s/it]\n",
            " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [1:06:38<08:04, 484.81s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [1:15:27<00:00, 498.30s/it]\n",
            "                                                  \n",
            "\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [1:15:44<00:00, 498.30s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [1:15:44<00:00, 454.43s/it]\n",
            "\n",
            "Log file: tour_tutorial/logs/2025_02_04_11_03_17_652_0.stdout\n",
            "\"Hello, World!\"\n",
            "\n",
            "@@@@@@@@@@@@@@@@@@@\n",
            "@                 @\n",
            "@   @@@@@  @  @   @\n",
            "@   @   @  @  @   @\n",
            "@   @@@@@  @@@@   @\n",
            "@                 @\n",
            "@   @@@@@@@   @   @\n",
            "@   @  @  @   @   @\n",
            "@   @  @  @   @   @\n",
            "@                 @\n",
            "@@@@@@@@@@@@@@@@@@@\n",
            "\n",
            "[2025-02-04 11:03:30,735][oumi][rank0][pid:82345][MainThread][INFO]][distributed.py:546] Setting random seed to 42 on rank 0.\n",
            "[2025-02-04 11:03:45,410][oumi][rank0][pid:82345][MainThread][INFO]][torch_utils.py:66] Torch version: 2.4.1+cu121. NumPy version: 1.26.4\n",
            "[2025-02-04 11:03:45,410][oumi][rank0][pid:82345][MainThread][INFO]][torch_utils.py:68] CUDA is not available!\n",
            "[2025-02-04 11:03:45,415][oumi][rank0][pid:82345][MainThread][INFO]][train.py:133] Oumi version: 0.1.3\n",
            "[2025-02-04 11:03:45,420][oumi][rank0][pid:82345][MainThread][INFO]][train.py:174] TrainingConfig:\n",
            "TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='yahma/alpaca-cleaned',\n",
            "                                                                                dataset_path=None,\n",
            "                                                                                subset=None,\n",
            "                                                                                split='train',\n",
            "                                                                                dataset_kwargs={},\n",
            "                                                                                sample_count=None,\n",
            "                                                                                mixture_proportion=None,\n",
            "                                                                                shuffle=False,\n",
            "                                                                                seed=None,\n",
            "                                                                                shuffle_buffer_size=1000,\n",
            "                                                                                trust_remote_code=False,\n",
            "                                                                                transform_num_workers=None)],\n",
            "                                                        collator_name=None,\n",
            "                                                        pack=False,\n",
            "                                                        stream=False,\n",
            "                                                        target_col='prompt',\n",
            "                                                        mixture_strategy='first_exhausted',\n",
            "                                                        seed=None,\n",
            "                                                        use_async_dataset=False,\n",
            "                                                        use_torchdata=None),\n",
            "                               test=DatasetSplitParams(datasets=[],\n",
            "                                                       collator_name=None,\n",
            "                                                       pack=False,\n",
            "                                                       stream=False,\n",
            "                                                       target_col=None,\n",
            "                                                       mixture_strategy='first_exhausted',\n",
            "                                                       seed=None,\n",
            "                                                       use_async_dataset=False,\n",
            "                                                       use_torchdata=None),\n",
            "                               validation=DatasetSplitParams(datasets=[],\n",
            "                                                             collator_name=None,\n",
            "                                                             pack=False,\n",
            "                                                             stream=False,\n",
            "                                                             target_col=None,\n",
            "                                                             mixture_strategy='first_exhausted',\n",
            "                                                             seed=None,\n",
            "                                                             use_async_dataset=False,\n",
            "                                                             use_torchdata=None)),\n",
            "               model=ModelParams(model_name='HuggingFaceTB/SmolLM2-135M-Instruct',\n",
            "                                 adapter_model=None,\n",
            "                                 tokenizer_name=None,\n",
            "                                 tokenizer_pad_token=None,\n",
            "                                 tokenizer_kwargs={},\n",
            "                                 model_max_length=None,\n",
            "                                 load_pretrained_weights=True,\n",
            "                                 trust_remote_code=True,\n",
            "                                 torch_dtype_str='bfloat16',\n",
            "                                 compile=False,\n",
            "                                 chat_template=None,\n",
            "                                 attn_implementation=None,\n",
            "                                 device_map='auto',\n",
            "                                 model_kwargs={},\n",
            "                                 enable_liger_kernel=False,\n",
            "                                 shard_for_eval=False,\n",
            "                                 freeze_layers=[]),\n",
            "               training=TrainingParams(use_peft=False,\n",
            "                                       trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>,\n",
            "                                       enable_gradient_checkpointing=False,\n",
            "                                       gradient_checkpointing_kwargs={},\n",
            "                                       output_dir='tour_tutorial/output',\n",
            "                                       per_device_train_batch_size=2,\n",
            "                                       per_device_eval_batch_size=8,\n",
            "                                       gradient_accumulation_steps=1,\n",
            "                                       max_steps=10,\n",
            "                                       num_train_epochs=3,\n",
            "                                       save_epoch=False,\n",
            "                                       save_steps=500,\n",
            "                                       save_final_model=True,\n",
            "                                       seed=42,\n",
            "                                       run_name='smollm2_135m_sft',\n",
            "                                       metrics_function=None,\n",
            "                                       log_level='info',\n",
            "                                       dep_log_level='warning',\n",
            "                                       enable_wandb=False,\n",
            "                                       enable_tensorboard=True,\n",
            "                                       logging_strategy='steps',\n",
            "                                       logging_dir=None,\n",
            "                                       logging_steps=50,\n",
            "                                       logging_first_step=False,\n",
            "                                       eval_strategy='no',\n",
            "                                       eval_steps=500,\n",
            "                                       learning_rate=5e-05,\n",
            "                                       lr_scheduler_type='linear',\n",
            "                                       lr_scheduler_kwargs={},\n",
            "                                       warmup_ratio=None,\n",
            "                                       warmup_steps=None,\n",
            "                                       optimizer='adamw_torch',\n",
            "                                       weight_decay=0.0,\n",
            "                                       adam_beta1=0.9,\n",
            "                                       adam_beta2=0.999,\n",
            "                                       adam_epsilon=1e-08,\n",
            "                                       sgd_momentum=0.0,\n",
            "                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,\n",
            "                                       compile=False,\n",
            "                                       include_performance_metrics=False,\n",
            "                                       include_alternative_mfu_metrics=False,\n",
            "                                       log_model_summary=False,\n",
            "                                       resume_from_checkpoint=None,\n",
            "                                       try_resume_from_last_checkpoint=False,\n",
            "                                       dataloader_num_workers=0,\n",
            "                                       dataloader_prefetch_factor=None,\n",
            "                                       dataloader_main_process_only=None,\n",
            "                                       ddp_find_unused_parameters=None,\n",
            "                                       max_grad_norm=1.0,\n",
            "                                       trainer_kwargs={},\n",
            "                                       profiler=ProfilerParams(save_dir=None,\n",
            "                                                               enable_cpu_profiling=False,\n",
            "                                                               enable_cuda_profiling=False,\n",
            "                                                               record_shapes=False,\n",
            "                                                               profile_memory=False,\n",
            "                                                               with_stack=False,\n",
            "                                                               with_flops=False,\n",
            "                                                               with_modules=False,\n",
            "                                                               row_limit=50,\n",
            "                                                               schedule=ProfilerScheduleParams(enable_schedule=False,\n",
            "                                                                                               wait=0,\n",
            "                                                                                               warmup=1,\n",
            "                                                                                               active=3,\n",
            "                                                                                               repeat=1,\n",
            "                                                                                               skip_first=1)),\n",
            "                                       telemetry=TelemetryParams(telemetry_dir='telemetry',\n",
            "                                                                 collect_telemetry_for_all_ranks=False,\n",
            "                                                                 track_gpu_temperature=False),\n",
            "                                       empty_device_cache_steps=None,\n",
            "                                       nccl_default_timeout_minutes=None),\n",
            "               peft=PeftParams(lora_r=8,\n",
            "                               lora_alpha=8,\n",
            "                               lora_dropout=0.0,\n",
            "                               lora_target_modules=None,\n",
            "                               lora_modules_to_save=None,\n",
            "                               lora_bias='none',\n",
            "                               lora_init_weights=<LoraWeightInitialization.DEFAULT: 'default'>,\n",
            "                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
            "                               q_lora=False,\n",
            "                               q_lora_bits=4,\n",
            "                               bnb_4bit_quant_type='fp4',\n",
            "                               use_bnb_nested_quant=False,\n",
            "                               bnb_4bit_quant_storage='uint8',\n",
            "                               bnb_4bit_compute_dtype='float32',\n",
            "                               peft_save_mode=<PeftSaveMode.ADAPTER_ONLY: 'adapter_only'>),\n",
            "               fsdp=FSDPParams(enable_fsdp=False,\n",
            "                               sharding_strategy=<ShardingStrategy.FULL_SHARD: 'FULL_SHARD'>,\n",
            "                               cpu_offload=False,\n",
            "                               mixed_precision=None,\n",
            "                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,\n",
            "                               forward_prefetch=False,\n",
            "                               use_orig_params=None,\n",
            "                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,\n",
            "                               auto_wrap_policy=<AutoWrapPolicy.NO_WRAP: 'NO_WRAP'>,\n",
            "                               min_num_params=100000,\n",
            "                               transformer_layer_cls=None,\n",
            "                               sync_module_states=True))\n",
            "[2025-02-04 11:03:46,024][oumi][rank0][pid:82345][MainThread][INFO]][models.py:185] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
            "[2025-02-04 11:03:46,024][oumi][rank0][pid:82345][MainThread][INFO]][models.py:255] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
            "[2025-02-04 11:03:47,392][oumi][rank0][pid:82345][MainThread][INFO]][base_map_dataset.py:68] Creating map dataset (type: AlpacaDataset) dataset_name: 'yahma/alpaca-cleaned', dataset_path: 'None'...\n",
            "[2025-02-04 11:03:49,191][oumi][rank0][pid:82345][MainThread][INFO]][base_map_dataset.py:472] Dataset Info:\n",
            "\tSplit: train\n",
            "\tVersion: 0.0.0\n",
            "\tDataset size: 40283906\n",
            "\tDownload size: 44307561\n",
            "\tSize: 84591467 bytes\n",
            "\tRows: 51760\n",
            "\tColumns: ['output', 'input', 'instruction']\n",
            "[2025-02-04 11:03:49,926][oumi][rank0][pid:82345][MainThread][INFO]][base_map_dataset.py:411] Loaded DataFrame with shape: (51760, 3). Columns:\n",
            "output         object\n",
            "input          object\n",
            "instruction    object\n",
            "dtype: object\n",
            "[2025-02-04 11:03:50,033][oumi][rank0][pid:82345][MainThread][INFO]][base_map_dataset.py:297] AlpacaDataset: features=dict_keys(['input_ids', 'attention_mask'])\n",
            "[2025-02-04 11:03:51,315][oumi][rank0][pid:82345][MainThread][INFO]][base_map_dataset.py:361] Finished transforming dataset (AlpacaDataset)! Speed: 40401.16 examples/sec. Examples: 51760. Duration: 1.3 sec. Transform workers: 1.\n",
            "[2025-02-04 11:03:51,649][oumi][rank0][pid:82345][MainThread][INFO]][torch_profiler_utils.py:150] PROF: Torch Profiler disabled!\n",
            "[2025-02-04 11:03:51,652][oumi][rank0][pid:82345][MainThread][INFO]][training.py:49] SFTConfig(output_dir='tour_tutorial/output',\n",
            "          overwrite_output_dir=False,\n",
            "          do_train=False,\n",
            "          do_eval=False,\n",
            "          do_predict=False,\n",
            "          eval_strategy=<IntervalStrategy.NO: 'no'>,\n",
            "          prediction_loss_only=False,\n",
            "          per_device_train_batch_size=2,\n",
            "          per_device_eval_batch_size=8,\n",
            "          per_gpu_train_batch_size=None,\n",
            "          per_gpu_eval_batch_size=None,\n",
            "          gradient_accumulation_steps=1,\n",
            "          eval_accumulation_steps=None,\n",
            "          eval_delay=0,\n",
            "          torch_empty_cache_steps=None,\n",
            "          learning_rate=5e-05,\n",
            "          weight_decay=0.0,\n",
            "          adam_beta1=0.9,\n",
            "          adam_beta2=0.999,\n",
            "          adam_epsilon=1e-08,\n",
            "          max_grad_norm=1.0,\n",
            "          num_train_epochs=3,\n",
            "          max_steps=10,\n",
            "          lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>,\n",
            "          lr_scheduler_kwargs={},\n",
            "          warmup_ratio=0.0,\n",
            "          warmup_steps=0,\n",
            "          log_level='warning',\n",
            "          log_level_replica='warning',\n",
            "          log_on_each_node=True,\n",
            "          logging_dir='tour_tutorial/output/runs/Feb04_11-03-51_3230a9be9608',\n",
            "          logging_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
            "          logging_first_step=False,\n",
            "          logging_steps=50,\n",
            "          logging_nan_inf_filter=True,\n",
            "          save_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
            "          save_steps=500,\n",
            "          save_total_limit=None,\n",
            "          save_safetensors=True,\n",
            "          save_on_each_node=False,\n",
            "          save_only_model=False,\n",
            "          restore_callback_states_from_checkpoint=False,\n",
            "          no_cuda=False,\n",
            "          use_cpu=False,\n",
            "          use_mps_device=False,\n",
            "          seed=42,\n",
            "          data_seed=None,\n",
            "          jit_mode_eval=False,\n",
            "          use_ipex=False,\n",
            "          bf16=False,\n",
            "          fp16=False,\n",
            "          fp16_opt_level='O1',\n",
            "          half_precision_backend='auto',\n",
            "          bf16_full_eval=False,\n",
            "          fp16_full_eval=False,\n",
            "          tf32=None,\n",
            "          local_rank=0,\n",
            "          ddp_backend=None,\n",
            "          tpu_num_cores=None,\n",
            "          tpu_metrics_debug=False,\n",
            "          debug=[],\n",
            "          dataloader_drop_last=False,\n",
            "          eval_steps=500,\n",
            "          dataloader_num_workers=0,\n",
            "          dataloader_prefetch_factor=None,\n",
            "          past_index=-1,\n",
            "          run_name='smollm2_135m_sft',\n",
            "          disable_tqdm=False,\n",
            "          remove_unused_columns=True,\n",
            "          label_names=None,\n",
            "          load_best_model_at_end=False,\n",
            "          metric_for_best_model=None,\n",
            "          greater_is_better=None,\n",
            "          ignore_data_skip=False,\n",
            "          fsdp=[],\n",
            "          fsdp_min_num_params=0,\n",
            "          fsdp_config={'min_num_params': 0,\n",
            "                       'xla': False,\n",
            "                       'xla_fsdp_grad_ckpt': False,\n",
            "                       'xla_fsdp_v2': False},\n",
            "          fsdp_transformer_layer_cls_to_wrap=None,\n",
            "          accelerator_config=AcceleratorConfig(split_batches=False,\n",
            "                                               dispatch_batches=None,\n",
            "                                               even_batches=True,\n",
            "                                               use_seedable_sampler=True,\n",
            "                                               non_blocking=False,\n",
            "                                               gradient_accumulation_kwargs=None,\n",
            "                                               use_configured_state=False),\n",
            "          deepspeed=None,\n",
            "          label_smoothing_factor=0.0,\n",
            "          optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>,\n",
            "          optim_args=None,\n",
            "          adafactor=False,\n",
            "          group_by_length=False,\n",
            "          length_column_name='length',\n",
            "          report_to=['tensorboard'],\n",
            "          ddp_find_unused_parameters=None,\n",
            "          ddp_bucket_cap_mb=None,\n",
            "          ddp_broadcast_buffers=None,\n",
            "          dataloader_pin_memory=True,\n",
            "          dataloader_persistent_workers=False,\n",
            "          skip_memory_metrics=True,\n",
            "          use_legacy_prediction_loop=False,\n",
            "          push_to_hub=False,\n",
            "          resume_from_checkpoint=None,\n",
            "          hub_model_id=None,\n",
            "          hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>,\n",
            "          hub_token=None,\n",
            "          hub_private_repo=False,\n",
            "          hub_always_push=False,\n",
            "          gradient_checkpointing=False,\n",
            "          gradient_checkpointing_kwargs={},\n",
            "          include_inputs_for_metrics=False,\n",
            "          eval_do_concat_batches=True,\n",
            "          fp16_backend='auto',\n",
            "          evaluation_strategy=None,\n",
            "          push_to_hub_model_id=None,\n",
            "          push_to_hub_organization=None,\n",
            "          push_to_hub_token=None,\n",
            "          mp_parameters='',\n",
            "          auto_find_batch_size=False,\n",
            "          full_determinism=False,\n",
            "          torchdynamo=None,\n",
            "          ray_scope='last',\n",
            "          ddp_timeout=1800,\n",
            "          torch_compile=False,\n",
            "          torch_compile_backend=None,\n",
            "          torch_compile_mode=None,\n",
            "          dispatch_batches=None,\n",
            "          split_batches=None,\n",
            "          include_tokens_per_second=False,\n",
            "          include_num_input_tokens_seen=False,\n",
            "          neftune_noise_alpha=None,\n",
            "          optim_target_modules=None,\n",
            "          batch_eval_metrics=False,\n",
            "          eval_on_start=False,\n",
            "          use_liger_kernel=False,\n",
            "          eval_use_gather_object=False,\n",
            "          dataset_text_field=None,\n",
            "          packing=False,\n",
            "          max_seq_length=None,\n",
            "          dataset_num_proc=None,\n",
            "          dataset_batch_size=1000,\n",
            "          model_init_kwargs=None,\n",
            "          dataset_kwargs=None,\n",
            "          eval_packing=None,\n",
            "          num_of_sequences=1024,\n",
            "          chars_per_token=3.6,\n",
            "          use_liger=False)\n",
            "[2025-02-04 11:03:51,676][oumi][rank0][pid:82345][MainThread][INFO]][device_utils.py:283] GPU Metrics Before Training: GPU runtime info: None.\n",
            "[2025-02-04 11:03:51,676][oumi][rank0][pid:82345][MainThread][INFO]][train.py:312] Training init time: 6.267s\n",
            "[2025-02-04 11:03:51,676][oumi][rank0][pid:82345][MainThread][INFO]][train.py:313] Starting training... (TrainerType.TRL_SFT, transformers: 4.45.2)\n",
            "{'train_runtime': 4544.3485, 'train_samples_per_second': 0.004, 'train_steps_per_second': 0.002, 'train_loss': 2.0375930786132814, 'epoch': 0.0}\n",
            "[2025-02-04 12:19:36,364][oumi][rank0][pid:82345][MainThread][INFO]][train.py:320] Training is Complete.\n",
            "[2025-02-04 12:19:36,364][oumi][rank0][pid:82345][MainThread][INFO]][device_utils.py:283] GPU Metrics After Training: GPU runtime info: None.\n",
            "[2025-02-04 12:19:36,364][oumi][rank0][pid:82345][MainThread][INFO]][train.py:327] Saving final state...\n",
            "[2025-02-04 12:19:36,365][oumi][rank0][pid:82345][MainThread][INFO]][train.py:332] Saving final model...\n",
            "[2025-02-04 12:19:40,654][oumi][rank0][pid:82345][MainThread][INFO]][hf_trainer.py:102] Model has been saved at tour_tutorial/output\n",
            "[2025-02-04 12:19:40,654][oumi][rank0][pid:82345][MainThread][INFO]][train.py:339] \n",
            "\n",
            "¬ª We're always looking for feedback. What's one thing we can improve? https://oumi.ai/feedback\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for log_file in Path(logs_dir).iterdir():\n",
        "    print(f\"Log file: {log_file}\")\n",
        "    with open(log_file) as f:\n",
        "        print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnWJ_XgiSgtT"
      },
      "source": [
        "# ‚öôÔ∏è Customizing Datasets and Clusters\n",
        "\n",
        "Oumi offers rich customization that allows users to build custom solutions on top of our existing building blocks. Several of Oumi's primary resources (Datasets, Clouds, etc.) leverage the Oumi Registry when invoked.\n",
        "\n",
        "This registry allows users to build custom classes that function as drop-in replacements for core functionality.\n",
        "\n",
        "For more details on registering custom datasets, see the [tutorial here](https://github.com/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Datasets%20Tutorial.ipynb).\n",
        "\n",
        "For a tutorial on writing a custom cloud/cluster for running jobs, see the [tutorial here](https://github.com/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Launching%20Jobs%20on%20Custom%20Clusters.ipynb).\n",
        "\n",
        "You can find further information about the required registry decorators [here](https://oumi.ai/docs/en/latest/api/oumi.core.registry.html#oumi.core.registry.register_cloud_builder)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcM7KZ2OSgtT"
      },
      "source": [
        "# üß≠ What's Next?\n",
        "\n",
        "Now that you've completed the basic tour, you're ready to tackle the other [notebook guides & tutorials](https://oumi.ai/docs/en/latest/get_started/tutorials.html).\n",
        "\n",
        "If you have not already, make sure to take a look at the [Quickstart](https://oumi.ai/docs/en/latest/get_started/quickstart.html) for an overview of our CLI."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "oumi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}